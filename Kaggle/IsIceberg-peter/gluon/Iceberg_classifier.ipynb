{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from subprocess import check_output\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604\n"
     ]
    }
   ],
   "source": [
    "data_path = 'D:/dataset/kaggle_Iceberg/'\n",
    "train_path = data_path + \"train.json\"\n",
    "test_path  = data_path + \"test.json\"\n",
    "train_json = pd.read_json(train_path)\n",
    "test_json = pd.read_json(test_path)\n",
    "print (len(train_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>band_1</th>\n",
       "      <th>band_2</th>\n",
       "      <th>id</th>\n",
       "      <th>inc_angle</th>\n",
       "      <th>is_iceberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-27.878361, -27.15416, -28.668615, -29.537971...</td>\n",
       "      <td>[-27.154118, -29.537888, -31.0306, -32.190483,...</td>\n",
       "      <td>dfd5f913</td>\n",
       "      <td>43.9239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-12.242375, -14.920305, -14.920363, -12.66633...</td>\n",
       "      <td>[-31.506321, -27.984554, -26.645678, -23.76760...</td>\n",
       "      <td>e25388fd</td>\n",
       "      <td>38.1562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-24.603676, -24.603714, -24.871029, -23.15277...</td>\n",
       "      <td>[-24.870956, -24.092632, -20.653963, -19.41104...</td>\n",
       "      <td>58b2aaa0</td>\n",
       "      <td>45.2859</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-22.454607, -23.082819, -23.998013, -23.99805...</td>\n",
       "      <td>[-27.889421, -27.519794, -27.165262, -29.10350...</td>\n",
       "      <td>4cfc3a18</td>\n",
       "      <td>43.8306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-26.006956, -23.164886, -23.164886, -26.89116...</td>\n",
       "      <td>[-27.206915, -30.259186, -30.259186, -23.16495...</td>\n",
       "      <td>271f93f4</td>\n",
       "      <td>35.6256</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              band_1  \\\n",
       "0  [-27.878361, -27.15416, -28.668615, -29.537971...   \n",
       "1  [-12.242375, -14.920305, -14.920363, -12.66633...   \n",
       "2  [-24.603676, -24.603714, -24.871029, -23.15277...   \n",
       "3  [-22.454607, -23.082819, -23.998013, -23.99805...   \n",
       "4  [-26.006956, -23.164886, -23.164886, -26.89116...   \n",
       "\n",
       "                                              band_2        id inc_angle  \\\n",
       "0  [-27.154118, -29.537888, -31.0306, -32.190483,...  dfd5f913   43.9239   \n",
       "1  [-31.506321, -27.984554, -26.645678, -23.76760...  e25388fd   38.1562   \n",
       "2  [-24.870956, -24.092632, -20.653963, -19.41104...  58b2aaa0   45.2859   \n",
       "3  [-27.889421, -27.519794, -27.165262, -29.10350...  4cfc3a18   43.8306   \n",
       "4  [-27.206915, -30.259186, -30.259186, -23.16495...  271f93f4   35.6256   \n",
       "\n",
       "   is_iceberg  \n",
       "0           0  \n",
       "1           0  \n",
       "2           1  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Train Records\")\n",
    "train_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Records\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>band_1</th>\n",
       "      <th>band_2</th>\n",
       "      <th>id</th>\n",
       "      <th>inc_angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-15.863251, -15.201077, -17.887735, -19.17248...</td>\n",
       "      <td>[-21.629612, -21.142353, -23.908337, -28.34524...</td>\n",
       "      <td>5941774d</td>\n",
       "      <td>34.966400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-26.0589694977, -26.0589694977, -26.058969497...</td>\n",
       "      <td>[-25.7542076111, -25.7542076111, -25.754207611...</td>\n",
       "      <td>4023181e</td>\n",
       "      <td>32.615072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-14.1410999298, -15.0642414093, -17.375520706...</td>\n",
       "      <td>[-14.745639801, -14.5904102325, -14.3626976013...</td>\n",
       "      <td>b20200e4</td>\n",
       "      <td>37.505433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-12.167478, -13.706167, -16.54837, -13.572674...</td>\n",
       "      <td>[-24.32222, -26.375538, -24.096739, -23.8769, ...</td>\n",
       "      <td>e7f018bb</td>\n",
       "      <td>34.473900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-23.3745937347, -26.0271816254, -28.121963501...</td>\n",
       "      <td>[-25.7223434448, -27.0115776062, -23.149162292...</td>\n",
       "      <td>4371c8c3</td>\n",
       "      <td>43.918874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              band_1  \\\n",
       "0  [-15.863251, -15.201077, -17.887735, -19.17248...   \n",
       "1  [-26.0589694977, -26.0589694977, -26.058969497...   \n",
       "2  [-14.1410999298, -15.0642414093, -17.375520706...   \n",
       "3  [-12.167478, -13.706167, -16.54837, -13.572674...   \n",
       "4  [-23.3745937347, -26.0271816254, -28.121963501...   \n",
       "\n",
       "                                              band_2        id  inc_angle  \n",
       "0  [-21.629612, -21.142353, -23.908337, -28.34524...  5941774d  34.966400  \n",
       "1  [-25.7542076111, -25.7542076111, -25.754207611...  4023181e  32.615072  \n",
       "2  [-14.745639801, -14.5904102325, -14.3626976013...  b20200e4  37.505433  \n",
       "3  [-24.32222, -26.375538, -24.096739, -23.8769, ...  e7f018bb  34.473900  \n",
       "4  [-25.7223434448, -27.0115776062, -23.149162292...  4371c8c3  43.918874  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test Records\")\n",
    "test_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.data import vision\n",
    "\n",
    "def transform_train(data, label):\n",
    "    im = image.imresize(data.astype('float32') / 255, 75, 75)\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 75, 75), resize=0,\n",
    "                        rand_crop=True, rand_resize=False, rand_mirror=False,\n",
    "                        mean=None, std=None,\n",
    "                        brightness=0, contrast=0,\n",
    "                        saturation=0, hue=0,\n",
    "                        pca_noise=0, rand_gray=0, inter_method=2)\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    # 将数据格式从\"高*宽*通道\"改为\"通道*高*宽\"。\n",
    "    im = nd.transpose(im, (2,0,1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))\n",
    "\n",
    "def transform_test(data, label):\n",
    "    im = image.imresize(data.astype('float32') / 255, 75, 755)\n",
    "    im = nd.transpose(im, (2,0,1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_norm(data):\n",
    "    images = []\n",
    "    for i,row in data.iterrows():\n",
    "        band_1 = np.reshape(np.array(row[\"band_1\"]).astype(np.float32),(75,75))\n",
    "        band_2 = np.reshape(np.array(row[\"band_2\"]).astype(np.float32),(75,75))\n",
    "        band_3 = band_1+band_2\n",
    "\n",
    "        band_1_norm = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
    "        band_2_norm = (band_2 - band_2. mean()) / (band_2.max() - band_2.min())\n",
    "        band_3_norm = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n",
    "        images.append(np.stack((band_1_norm,band_2_norm,band_3_norm)))\n",
    "    return np.array(images)\n",
    "\n",
    "X_train_total = data_norm(train_json)\n",
    "y_train_total = np.array(train_json['is_iceberg'])\n",
    "X_angle_train_total = np.array(train_json.inc_angle)\n",
    "\n",
    "X_test = data_norm(test_json)\n",
    "X_angle_test = np.array(test_json.inc_angle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Fix and Preprocess Data\n",
    "# x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train_json[\"band_1\"]])\n",
    "# x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train_json[\"band_2\"]])\n",
    "\n",
    "# X_train_total = np.concatenate([x_band1[:, :, :, np.newaxis]\n",
    "#                           , x_band2[:, :, :, np.newaxis]\n",
    "#                          , ((x_band1+x_band1)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "# X_angle_train_total = np.array(train_json.inc_angle)\n",
    "# y_train_total = np.array(train_json[\"is_iceberg\"],dtype='float')\n",
    "# X_train_total = X_train_total.transpose((0,3,1,2))\n",
    "\n",
    "\n",
    "# # Test data\n",
    "# x_band1 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test_json[\"band_1\"]])\n",
    "# x_band2 = np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in test_json[\"band_2\"]])\n",
    "# X_test = np.concatenate([x_band1[:, :, :, np.newaxis]\n",
    "#                           , x_band2[:, :, :, np.newaxis]\n",
    "#                          , ((x_band1+x_band1)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "# X_angle_test = np.array(test_json.inc_angle)\n",
    "# X_test = X_test.transpose((0,3,1,2))\n",
    "\n",
    "\n",
    "X_train, X_valid, X_angle_train, X_angle_valid, y_train, y_valid = train_test_split(X_train_total\n",
    "                    , X_angle_train_total, y_train_total, random_state=123, test_size=0.1)\n",
    "\n",
    "    \n",
    "\n",
    "y_test = np.zeros((X_test.shape[0],), dtype=np.int)\n",
    "\n",
    "train_ds = gluon.data.ArrayDataset(X_train, y_train)\n",
    "valid_ds = gluon.data.ArrayDataset(X_valid, y_valid)\n",
    "train_valid_ds = gluon.data.ArrayDataset(X_train_total,y_train_total)\n",
    "test_ds = gluon.data.ArrayDataset(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1443L, 3L, 75L, 75L)\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "loader = gluon.data.DataLoader\n",
    "train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "valid_data = loader(valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "train_valid_data= loader(train_valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "test_data = loader(test_ds,batch_size,shuffle = True,last_batch='keep')\n",
    "# 交叉熵损失函数。\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import nd\n",
    "\n",
    "\n",
    "def get_loss(data, net, ctx):\n",
    "    loss = 0.0\n",
    "    for feas, label in data:\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(feas.as_in_context(ctx))\n",
    "        cross_entropy = softmax_cross_entropy(output, label)\n",
    "        loss += nd.mean(cross_entropy).asscalar()\n",
    "    return loss / len(data)\n",
    "\n",
    "class Residual(nn.HybridBlock):\n",
    "    def __init__(self, channels, same_shape=True, **kwargs):\n",
    "        super(Residual, self).__init__(**kwargs)\n",
    "        self.same_shape = same_shape\n",
    "        with self.name_scope():\n",
    "            strides = 1 if same_shape else 2\n",
    "            self.conv1 = nn.Conv2D(channels, kernel_size=3, padding=1,\n",
    "                                  strides=strides)\n",
    "            self.bn1 = nn.BatchNorm()\n",
    "            self.conv2 = nn.Conv2D(channels, kernel_size=3, padding=1)\n",
    "            self.bn2 = nn.BatchNorm()\n",
    "            if not same_shape:\n",
    "                self.conv3 = nn.Conv2D(channels, kernel_size=1,\n",
    "                                      strides=strides)\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = F.sigmoid(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if not self.same_shape:\n",
    "            x = self.conv3(x)\n",
    "        return F.sigmoid(out + x)\n",
    "\n",
    "\n",
    "class ResNet(nn.HybridBlock):\n",
    "    def __init__(self, num_classes, verbose=False, **kwargs):\n",
    "        super(ResNet, self).__init__(**kwargs)\n",
    "        self.verbose = verbose\n",
    "        with self.name_scope():\n",
    "            net = self.net = nn.HybridSequential()\n",
    "            # 模块1\n",
    "            net.add(nn.Conv2D(channels=32, kernel_size=3, strides=1, padding=1))\n",
    "            net.add(nn.BatchNorm())\n",
    "            net.add(nn.Activation(activation='sigmoid'))\n",
    "            # 模块2\n",
    "            for _ in range(3):\n",
    "                net.add(Residual(channels=32))\n",
    "            # 模块3\n",
    "            net.add(Residual(channels=64, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=64))\n",
    "            # 模块4\n",
    "            net.add(Residual(channels=128, same_shape=False))\n",
    "            for _ in range(2):\n",
    "                net.add(Residual(channels=128))\n",
    "            # 模块5\n",
    "            net.add(nn.AvgPool2D(pool_size=8))\n",
    "            net.add(nn.Flatten())\n",
    "            net.add(nn.Dense(num_classes))\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        out = x\n",
    "        for i, b in enumerate(self.net):\n",
    "            out = b(out)\n",
    "            if self.verbose:\n",
    "                print('Block %d output: %s'%(i+1, out.shape))\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_net(ctx):\n",
    "    num_outputs = 2\n",
    "    net = ResNet(num_outputs)\n",
    "    net.initialize(ctx=ctx, init=init.Xavier())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, wd, ctx, lr_period,\n",
    "          lr_decay):\n",
    "    trainer = gluon.Trainer(\n",
    "        net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9,\n",
    "                                      'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    plt_train_loss = []\n",
    "    plt_valid_loss = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        if epoch < 81 and epoch % lr_period == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        if epoch > 81 and epoch % 10 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.4)\n",
    "        for data, label in train_data:\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(ctx))\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_loss = get_loss(valid_data, net, ctx)\n",
    "            epoch_str = (\"Epoch %d. Train loss: %f, Valid loss %f, \"\n",
    "                         % (epoch, train_loss / len(train_data), valid_loss))\n",
    "            plt_train_loss.append(train_loss / len(train_data))\n",
    "            plt_valid_loss.append(valid_loss)\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Train loss: %f, \"\n",
    "                         % (epoch, train_loss / len(train_data)))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str + ', lr ' + str(trainer.learning_rate))\n",
    "\n",
    "    # plot \n",
    "    if valid_data is not None:\n",
    "        plt.plot(plt_train_loss)\n",
    "        plt.plot(plt_valid_loss)\n",
    "        plt.legend(['train_loss','test_loss'])\n",
    "        plt.savefig(\"Loss22.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train loss: 0.692149, Valid loss 0.697753, Time 00:00:06, lr 0.001\n",
      "Epoch 1. Train loss: 0.687728, Valid loss 0.699125, Time 00:00:06, lr 0.001\n",
      "Epoch 2. Train loss: 0.683959, Valid loss 0.716411, Time 00:00:06, lr 0.001\n",
      "Epoch 3. Train loss: 0.681885, Valid loss 0.708590, Time 00:00:06, lr 0.001\n",
      "Epoch 4. Train loss: 0.677403, Valid loss 0.684905, Time 00:00:06, lr 0.001\n",
      "Epoch 5. Train loss: 0.670429, Valid loss 0.716177, Time 00:00:06, lr 0.001\n",
      "Epoch 6. Train loss: 0.662977, Valid loss 0.689975, Time 00:00:06, lr 0.001\n",
      "Epoch 7. Train loss: 0.650112, Valid loss 0.731078, Time 00:00:06, lr 0.001\n",
      "Epoch 8. Train loss: 0.634836, Valid loss 0.767942, Time 00:00:06, lr 0.001\n",
      "Epoch 9. Train loss: 0.617269, Valid loss 0.870027, Time 00:00:06, lr 0.001\n",
      "Epoch 10. Train loss: 0.588467, Valid loss 1.143741, Time 00:00:06, lr 0.001\n",
      "Epoch 11. Train loss: 0.554011, Valid loss 1.696049, Time 00:00:06, lr 0.001\n",
      "Epoch 12. Train loss: 0.524092, Valid loss 1.573412, Time 00:00:06, lr 0.001\n",
      "Epoch 13. Train loss: 0.494817, Valid loss 1.124007, Time 00:00:06, lr 0.001\n",
      "Epoch 14. Train loss: 0.482974, Valid loss 0.495111, Time 00:00:06, lr 0.001\n",
      "Epoch 15. Train loss: 0.464085, Valid loss 1.382549, Time 00:00:06, lr 0.001\n",
      "Epoch 16. Train loss: 0.450175, Valid loss 1.586518, Time 00:00:06, lr 0.001\n",
      "Epoch 17. Train loss: 0.428678, Valid loss 2.208727, Time 00:00:06, lr 0.001\n",
      "Epoch 18. Train loss: 0.413408, Valid loss 2.049659, Time 00:00:06, lr 0.001\n",
      "Epoch 19. Train loss: 0.389811, Valid loss 2.309026, Time 00:00:06, lr 0.001\n",
      "Epoch 20. Train loss: 0.372174, Valid loss 1.326982, Time 00:00:06, lr 0.001\n",
      "Epoch 21. Train loss: 0.379670, Valid loss 2.737310, Time 00:00:06, lr 0.001\n",
      "Epoch 22. Train loss: 0.380985, Valid loss 1.953480, Time 00:00:06, lr 0.001\n",
      "Epoch 23. Train loss: 0.348013, Valid loss 0.515210, Time 00:00:06, lr 0.001\n",
      "Epoch 24. Train loss: 0.329586, Valid loss 0.873672, Time 00:00:06, lr 0.001\n",
      "Epoch 25. Train loss: 0.324236, Valid loss 1.727787, Time 00:00:06, lr 0.001\n",
      "Epoch 26. Train loss: 0.312436, Valid loss 0.924254, Time 00:00:06, lr 0.001\n",
      "Epoch 27. Train loss: 0.306842, Valid loss 1.231298, Time 00:00:06, lr 0.001\n",
      "Epoch 28. Train loss: 0.280317, Valid loss 1.350689, Time 00:00:06, lr 0.001\n",
      "Epoch 29. Train loss: 0.310020, Valid loss 2.432431, Time 00:00:06, lr 0.001\n",
      "Epoch 30. Train loss: 0.333241, Valid loss 1.548543, Time 00:00:06, lr 0.001\n",
      "Epoch 31. Train loss: 0.264081, Valid loss 2.308176, Time 00:00:06, lr 0.001\n",
      "Epoch 32. Train loss: 0.279127, Valid loss 0.475976, Time 00:00:06, lr 0.001\n",
      "Epoch 33. Train loss: 0.277015, Valid loss 1.476874, Time 00:00:06, lr 0.001\n",
      "Epoch 34. Train loss: 0.297500, Valid loss 3.123552, Time 00:00:06, lr 0.001\n",
      "Epoch 35. Train loss: 0.244225, Valid loss 4.769522, Time 00:00:06, lr 0.001\n",
      "Epoch 36. Train loss: 0.261425, Valid loss 2.133114, Time 00:00:06, lr 0.001\n",
      "Epoch 37. Train loss: 0.239532, Valid loss 2.911585, Time 00:00:06, lr 0.001\n",
      "Epoch 38. Train loss: 0.215746, Valid loss 2.170707, Time 00:00:06, lr 0.001\n",
      "Epoch 39. Train loss: 0.203753, Valid loss 1.184259, Time 00:00:06, lr 0.001\n",
      "Epoch 40. Train loss: 0.205079, Valid loss 1.975389, Time 00:00:06, lr 0.001\n",
      "Epoch 41. Train loss: 0.197923, Valid loss 3.423608, Time 00:00:06, lr 0.001\n",
      "Epoch 42. Train loss: 0.172886, Valid loss 3.534434, Time 00:00:06, lr 0.001\n",
      "Epoch 43. Train loss: 0.146868, Valid loss 1.210141, Time 00:00:06, lr 0.001\n",
      "Epoch 44. Train loss: 0.141540, Valid loss 3.739666, Time 00:00:06, lr 0.001\n",
      "Epoch 45. Train loss: 0.157754, Valid loss 4.657699, Time 00:00:06, lr 0.001\n",
      "Epoch 46. Train loss: 0.166025, Valid loss 3.341753, Time 00:00:06, lr 0.001\n",
      "Epoch 47. Train loss: 0.159798, Valid loss 2.615232, Time 00:00:06, lr 0.001\n",
      "Epoch 48. Train loss: 0.124809, Valid loss 0.878013, Time 00:00:06, lr 0.001\n",
      "Epoch 49. Train loss: 0.103237, Valid loss 3.382050, Time 00:00:06, lr 0.001\n",
      "Epoch 50. Train loss: 0.102287, Valid loss 1.661571, Time 00:00:06, lr 0.001\n",
      "Epoch 51. Train loss: 0.115076, Valid loss 5.293611, Time 00:00:06, lr 0.001\n",
      "Epoch 52. Train loss: 0.093325, Valid loss 5.753753, Time 00:00:06, lr 0.001\n",
      "Epoch 53. Train loss: 0.082048, Valid loss 3.634089, Time 00:00:06, lr 0.001\n",
      "Epoch 54. Train loss: 0.057228, Valid loss 4.118594, Time 00:00:06, lr 0.001\n",
      "Epoch 55. Train loss: 0.058248, Valid loss 0.401065, Time 00:00:06, lr 0.001\n",
      "Epoch 56. Train loss: 0.053666, Valid loss 3.554147, Time 00:00:06, lr 0.001\n",
      "Epoch 57. Train loss: 0.056299, Valid loss 4.291026, Time 00:00:06, lr 0.001\n",
      "Epoch 58. Train loss: 0.052587, Valid loss 6.116101, Time 00:00:06, lr 0.001\n",
      "Epoch 59. Train loss: 0.039754, Valid loss 2.951885, Time 00:00:06, lr 0.001\n",
      "Epoch 60. Train loss: 0.037531, Valid loss 2.699918, Time 00:00:06, lr 0.001\n",
      "Epoch 61. Train loss: 0.030020, Valid loss 5.141905, Time 00:00:06, lr 0.001\n",
      "Epoch 62. Train loss: 0.031167, Valid loss 4.257677, Time 00:00:06, lr 0.001\n",
      "Epoch 63. Train loss: 0.032009, Valid loss 5.422995, Time 00:00:06, lr 0.001\n",
      "Epoch 64. Train loss: 0.030343, Valid loss 5.834799, Time 00:00:06, lr 0.001\n",
      "Epoch 65. Train loss: 0.037947, Valid loss 5.466837, Time 00:00:06, lr 0.001\n",
      "Epoch 66. Train loss: 0.028322, Valid loss 5.024228, Time 00:00:06, lr 0.001\n",
      "Epoch 67. Train loss: 0.029270, Valid loss 6.225708, Time 00:00:06, lr 0.001\n",
      "Epoch 68. Train loss: 0.024481, Valid loss 1.922845, Time 00:00:06, lr 0.001\n",
      "Epoch 69. Train loss: 0.020511, Valid loss 1.887662, Time 00:00:06, lr 0.001\n",
      "Epoch 70. Train loss: 0.017594, Valid loss 1.243635, Time 00:00:06, lr 0.001\n",
      "Epoch 71. Train loss: 0.018413, Valid loss 0.403810, Time 00:00:06, lr 0.001\n",
      "Epoch 72. Train loss: 0.017805, Valid loss 3.620661, Time 00:00:06, lr 0.001\n",
      "Epoch 73. Train loss: 0.015707, Valid loss 4.790752, Time 00:00:06, lr 0.001\n",
      "Epoch 74. Train loss: 0.018892, Valid loss 4.478512, Time 00:00:06, lr 0.001\n",
      "Epoch 75. Train loss: 0.023120, Valid loss 2.927482, Time 00:00:06, lr 0.001\n",
      "Epoch 76. Train loss: 0.017011, Valid loss 2.969245, Time 00:00:06, lr 0.001\n",
      "Epoch 77. Train loss: 0.015098, Valid loss 2.630809, Time 00:00:06, lr 0.001\n",
      "Epoch 78. Train loss: 0.015150, Valid loss 2.556447, Time 00:00:06, lr 0.001\n",
      "Epoch 79. Train loss: 0.012016, Valid loss 6.006423, Time 00:00:06, lr 0.001\n",
      "Epoch 80. Train loss: 0.012542, Valid loss 1.257773, Time 00:00:06, lr 0.0001\n",
      "Epoch 81. Train loss: 0.022633, Valid loss 0.801371, Time 00:00:06, lr 0.0001\n",
      "Epoch 82. Train loss: 0.010773, Valid loss 1.178318, Time 00:00:06, lr 0.0001\n",
      "Epoch 83. Train loss: 0.011926, Valid loss 1.259857, Time 00:00:06, lr 0.0001\n",
      "Epoch 84. Train loss: 0.013530, Valid loss 0.297585, Time 00:00:06, lr 0.0001\n",
      "Epoch 85. Train loss: 0.012525, Valid loss 0.376971, Time 00:00:06, lr 0.0001\n",
      "Epoch 86. Train loss: 0.010508, Valid loss 0.351687, Time 00:00:06, lr 0.0001\n",
      "Epoch 87. Train loss: 0.010210, Valid loss 0.380424, Time 00:00:06, lr 0.0001\n",
      "Epoch 88. Train loss: 0.011180, Valid loss 0.406609, Time 00:00:06, lr 0.0001\n",
      "Epoch 89. Train loss: 0.010043, Valid loss 0.425608, Time 00:00:06, lr 0.0001\n",
      "Epoch 90. Train loss: 0.011041, Valid loss 0.331699, Time 00:00:06, lr 4e-05\n",
      "Epoch 91. Train loss: 0.010551, Valid loss 0.421602, Time 00:00:06, lr 4e-05\n",
      "Epoch 92. Train loss: 0.010188, Valid loss 0.281037, Time 00:00:06, lr 4e-05\n",
      "Epoch 93. Train loss: 0.011996, Valid loss 0.357968, Time 00:00:06, lr 4e-05\n",
      "Epoch 94. Train loss: 0.010711, Valid loss 0.264888, Time 00:00:06, lr 4e-05\n",
      "Epoch 95. Train loss: 0.009553, Valid loss 0.358924, Time 00:00:06, lr 4e-05\n",
      "Epoch 96. Train loss: 0.014789, Valid loss 0.263740, Time 00:00:06, lr 4e-05\n",
      "Epoch 97. Train loss: 0.010269, Valid loss 0.493257, Time 00:00:06, lr 4e-05\n",
      "Epoch 98. Train loss: 0.010156, Valid loss 0.425351, Time 00:00:06, lr 4e-05\n",
      "Epoch 99. Train loss: 0.011006, Valid loss 0.363157, Time 00:00:06, lr 4e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x41d9048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ctx = utils.try_gpu()\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "weight_decay = 5e-4\n",
    "lr_period = 80\n",
    "lr_decay = 0.1\n",
    "\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate,weight_decay, ctx, lr_period, lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train loss: 0.962804, Time 00:00:11, lr 0.001\n",
      "Epoch 1. Train loss: 0.736389, Time 00:00:07, lr 0.001\n",
      "Epoch 2. Train loss: 0.682343, Time 00:00:07, lr 0.001\n",
      "Epoch 3. Train loss: 0.668156, Time 00:00:07, lr 0.001\n",
      "Epoch 4. Train loss: 0.657714, Time 00:00:07, lr 0.001\n",
      "Epoch 5. Train loss: 0.645961, Time 00:00:07, lr 0.001\n",
      "Epoch 6. Train loss: 0.627884, Time 00:00:07, lr 0.001\n",
      "Epoch 7. Train loss: 0.608278, Time 00:00:07, lr 0.001\n",
      "Epoch 8. Train loss: 0.587128, Time 00:00:07, lr 0.001\n",
      "Epoch 9. Train loss: 0.564726, Time 00:00:07, lr 0.001\n",
      "Epoch 10. Train loss: 0.536573, Time 00:00:07, lr 0.001\n",
      "Epoch 11. Train loss: 0.507107, Time 00:00:07, lr 0.001\n",
      "Epoch 12. Train loss: 0.486011, Time 00:00:07, lr 0.001\n",
      "Epoch 13. Train loss: 0.460447, Time 00:00:07, lr 0.001\n",
      "Epoch 14. Train loss: 0.449548, Time 00:00:07, lr 0.001\n",
      "Epoch 15. Train loss: 0.420477, Time 00:00:07, lr 0.001\n",
      "Epoch 16. Train loss: 0.414294, Time 00:00:07, lr 0.001\n",
      "Epoch 17. Train loss: 0.398011, Time 00:00:07, lr 0.001\n",
      "Epoch 18. Train loss: 0.375415, Time 00:00:07, lr 0.001\n",
      "Epoch 19. Train loss: 0.358020, Time 00:00:07, lr 0.001\n",
      "Epoch 20. Train loss: 0.375270, Time 00:00:07, lr 0.001\n",
      "Epoch 21. Train loss: 0.350052, Time 00:00:07, lr 0.001\n",
      "Epoch 22. Train loss: 0.332353, Time 00:00:07, lr 0.001\n",
      "Epoch 23. Train loss: 0.315346, Time 00:00:07, lr 0.001\n",
      "Epoch 24. Train loss: 0.305195, Time 00:00:07, lr 0.001\n",
      "Epoch 25. Train loss: 0.299702, Time 00:00:07, lr 0.001\n",
      "Epoch 26. Train loss: 0.276777, Time 00:00:07, lr 0.001\n",
      "Epoch 27. Train loss: 0.286713, Time 00:00:07, lr 0.001\n",
      "Epoch 28. Train loss: 0.272537, Time 00:00:07, lr 0.001\n",
      "Epoch 29. Train loss: 0.260864, Time 00:00:07, lr 0.001\n",
      "Epoch 30. Train loss: 0.254796, Time 00:00:07, lr 0.001\n",
      "Epoch 31. Train loss: 0.247811, Time 00:00:07, lr 0.001\n",
      "Epoch 32. Train loss: 0.228920, Time 00:00:07, lr 0.001\n",
      "Epoch 33. Train loss: 0.218290, Time 00:00:07, lr 0.001\n",
      "Epoch 34. Train loss: 0.200507, Time 00:00:07, lr 0.001\n",
      "Epoch 35. Train loss: 0.227536, Time 00:00:07, lr 0.001\n",
      "Epoch 36. Train loss: 0.233383, Time 00:00:07, lr 0.001\n",
      "Epoch 37. Train loss: 0.208868, Time 00:00:07, lr 0.001\n",
      "Epoch 38. Train loss: 0.171729, Time 00:00:07, lr 0.001\n",
      "Epoch 39. Train loss: 0.177483, Time 00:00:07, lr 0.001\n",
      "Epoch 40. Train loss: 0.167616, Time 00:00:07, lr 0.001\n",
      "Epoch 41. Train loss: 0.147773, Time 00:00:07, lr 0.001\n",
      "Epoch 42. Train loss: 0.151042, Time 00:00:07, lr 0.001\n",
      "Epoch 43. Train loss: 0.146996, Time 00:00:07, lr 0.001\n",
      "Epoch 44. Train loss: 0.137548, Time 00:00:07, lr 0.001\n",
      "Epoch 45. Train loss: 0.135916, Time 00:00:07, lr 0.001\n",
      "Epoch 46. Train loss: 0.138310, Time 00:00:07, lr 0.001\n",
      "Epoch 47. Train loss: 0.100289, Time 00:00:07, lr 0.001\n",
      "Epoch 48. Train loss: 0.092699, Time 00:00:07, lr 0.001\n",
      "Epoch 49. Train loss: 0.099953, Time 00:00:07, lr 0.001\n",
      "Epoch 50. Train loss: 0.070090, Time 00:00:07, lr 0.001\n",
      "Epoch 51. Train loss: 0.068883, Time 00:00:07, lr 0.001\n",
      "Epoch 52. Train loss: 0.059140, Time 00:00:07, lr 0.001\n",
      "Epoch 53. Train loss: 0.056402, Time 00:00:07, lr 0.001\n",
      "Epoch 54. Train loss: 0.049528, Time 00:00:07, lr 0.001\n",
      "Epoch 55. Train loss: 0.043803, Time 00:00:07, lr 0.001\n",
      "Epoch 56. Train loss: 0.066127, Time 00:00:07, lr 0.001\n",
      "Epoch 57. Train loss: 0.058058, Time 00:00:07, lr 0.001\n",
      "Epoch 58. Train loss: 0.046621, Time 00:00:07, lr 0.001\n",
      "Epoch 59. Train loss: 0.035135, Time 00:00:07, lr 0.001\n",
      "Epoch 60. Train loss: 0.031657, Time 00:00:07, lr 0.001\n",
      "Epoch 61. Train loss: 0.029330, Time 00:00:07, lr 0.001\n",
      "Epoch 62. Train loss: 0.028198, Time 00:00:07, lr 0.001\n",
      "Epoch 63. Train loss: 0.029579, Time 00:00:07, lr 0.001\n",
      "Epoch 64. Train loss: 0.028705, Time 00:00:07, lr 0.001\n",
      "Epoch 65. Train loss: 0.022427, Time 00:00:07, lr 0.001\n",
      "Epoch 66. Train loss: 0.020800, Time 00:00:07, lr 0.001\n",
      "Epoch 67. Train loss: 0.026796, Time 00:00:07, lr 0.001\n",
      "Epoch 68. Train loss: 0.024950, Time 00:00:07, lr 0.001\n",
      "Epoch 69. Train loss: 0.029989, Time 00:00:07, lr 0.001\n",
      "Epoch 70. Train loss: 0.019196, Time 00:00:07, lr 0.001\n",
      "Epoch 71. Train loss: 0.017988, Time 00:00:07, lr 0.001\n",
      "Epoch 72. Train loss: 0.016963, Time 00:00:07, lr 0.001\n",
      "Epoch 73. Train loss: 0.017546, Time 00:00:07, lr 0.001\n",
      "Epoch 74. Train loss: 0.017452, Time 00:00:07, lr 0.001\n",
      "Epoch 75. Train loss: 0.013968, Time 00:00:07, lr 0.001\n",
      "Epoch 76. Train loss: 0.015660, Time 00:00:07, lr 0.001\n",
      "Epoch 77. Train loss: 0.016061, Time 00:00:07, lr 0.001\n",
      "Epoch 78. Train loss: 0.015954, Time 00:00:07, lr 0.001\n",
      "Epoch 79. Train loss: 0.011975, Time 00:00:07, lr 0.001\n",
      "Epoch 80. Train loss: 0.013743, Time 00:00:07, lr 0.0001\n",
      "Epoch 81. Train loss: 0.013116, Time 00:00:07, lr 0.0001\n",
      "Epoch 82. Train loss: 0.010110, Time 00:00:07, lr 0.0001\n",
      "Epoch 83. Train loss: 0.011148, Time 00:00:07, lr 0.0001\n",
      "Epoch 84. Train loss: 0.010950, Time 00:00:07, lr 0.0001\n",
      "Epoch 85. Train loss: 0.011362, Time 00:00:07, lr 0.0001\n",
      "Epoch 86. Train loss: 0.011308, Time 00:00:07, lr 0.0001\n",
      "Epoch 87. Train loss: 0.010241, Time 00:00:07, lr 0.0001\n",
      "Epoch 88. Train loss: 0.010040, Time 00:00:07, lr 0.0001\n",
      "Epoch 89. Train loss: 0.009987, Time 00:00:07, lr 0.0001\n",
      "Epoch 90. Train loss: 0.012032, Time 00:00:07, lr 4e-05\n",
      "Epoch 91. Train loss: 0.010377, Time 00:00:07, lr 4e-05\n",
      "Epoch 92. Train loss: 0.010151, Time 00:00:07, lr 4e-05\n",
      "Epoch 93. Train loss: 0.010140, Time 00:00:07, lr 4e-05\n",
      "Epoch 94. Train loss: 0.009121, Time 00:00:07, lr 4e-05\n",
      "Epoch 95. Train loss: 0.010739, Time 00:00:07, lr 4e-05\n",
      "Epoch 96. Train loss: 0.008986, Time 00:00:07, lr 4e-05\n",
      "Epoch 97. Train loss: 0.009697, Time 00:00:07, lr 4e-05\n",
      "Epoch 98. Train loss: 0.009263, Time 00:00:07, lr 4e-05\n",
      "Epoch 99. Train loss: 0.009859, Time 00:00:07, lr 4e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "net = get_net(ctx)\n",
    "net.hybridize()\n",
    "train(net, train_valid_data, None, num_epochs, learning_rate, weight_decay,\n",
    "      ctx, lr_period, lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for data, label in test_data:\n",
    "    output = nd.softmax(net(data.as_in_context(ctx)))\n",
    "    outputs.extend(output.asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8424\n"
     ]
    }
   ],
   "source": [
    "test_pre = []\n",
    "\n",
    "for num in outputs:\n",
    "    test_pre.append(num[1])\n",
    "print len(test_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': test_json[\"id\"], 'is_iceberg': test_pre})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
