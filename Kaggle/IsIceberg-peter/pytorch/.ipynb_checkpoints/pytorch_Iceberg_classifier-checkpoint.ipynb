{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yijie/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import torch\n",
    "import sys\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, log_loss, roc_auc_score, roc_curve, auc\n",
    "from sklearn.cross_validation import StratifiedKFold, ShuffleSplit, cross_val_score, train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 2.7.13 |Anaconda 4.3.0 (64-bit)| (default, Dec 20 2016, 23:09:15) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "__pyTorch VERSION: 0.2.0_3\n"
     ]
    }
   ],
   "source": [
    "print('__Python VERSION:', sys.version)\n",
    "print('__pyTorch VERSION:', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    num_workers = 0 # for windows version of PyTorch which does not share GPU tensors\n",
    "else:\n",
    "    num_workers = 4\n",
    "\n",
    "global_seed=999\n",
    "# Data params\n",
    "TARGET_VAR = 'target'\n",
    "\n",
    "# BASE_FOLDER = '../input/'\n",
    "BASE_FOLDER = '/media/yijie/文档/dataset/kaggle_Iceberg'\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "validationRatio = 0.11\n",
    "LR = 0.0005\n",
    "MOMENTUM = 0.95\n",
    "global_seed=999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import Iceberg_dataset\n",
    "import ice_models\n",
    "import iceberg_classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def fixSeed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def savePred(df_pred, val_score):\n",
    "    csv_path = str(val_score) + '_sample_submission.csv'\n",
    "    df_pred.to_csv(csv_path, columns=('id', 'is_iceberg'), index=None)\n",
    "    print(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fixSeed(global_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensamble number:0\n",
      "TRAIN Loss: 0.110239\n",
      "VALIDATION Loss: 0.330469\n",
      "Ensamble number:1\n",
      "TRAIN Loss: 0.127133\n",
      "VALIDATION Loss: 0.242808\n",
      "Ensamble number:2\n",
      "TRAIN Loss: 0.110295\n",
      "VALIDATION Loss: 0.210377\n",
      "Ensamble number:3\n",
      "TRAIN Loss: 0.107380\n",
      "VALIDATION Loss: 0.237858\n",
      "Ensamble number:4\n",
      "TRAIN Loss: 0.101408\n",
      "VALIDATION Loss: 0.227866\n",
      "Ensamble number:5\n",
      "TRAIN Loss: 0.121240\n",
      "VALIDATION Loss: 0.408643\n",
      "Ensamble number:6\n",
      "TRAIN Loss: 0.148860\n",
      "VALIDATION Loss: 0.216407\n",
      "Ensamble number:7\n",
      "TRAIN Loss: 0.117843\n",
      "VALIDATION Loss: 0.216162\n",
      "Ensamble number:8\n",
      "TRAIN Loss: 0.119644\n",
      "VALIDATION Loss: 0.210134\n",
      "Ensamble number:9\n",
      "TRAIN Loss: 0.096856\n",
      "VALIDATION Loss: 0.275787\n",
      "Ensamble number:10\n",
      "TRAIN Loss: 0.108073\n",
      "VALIDATION Loss: 0.212838\n",
      "Ensamble number:11\n",
      "TRAIN Loss: 0.179635\n",
      "VALIDATION Loss: 0.221806\n",
      "Ensamble number:12\n",
      "TRAIN Loss: 0.119883\n",
      "VALIDATION Loss: 0.234434\n",
      "Ensamble number:13\n",
      "TRAIN Loss: 0.133515\n",
      "VALIDATION Loss: 0.211205\n",
      "Ensamble number:14\n",
      "TRAIN Loss: 0.096323\n",
      "VALIDATION Loss: 0.255439\n",
      "Ensamble number:15\n",
      "TRAIN Loss: 0.135261\n",
      "VALIDATION Loss: 0.210711\n",
      "Ensamble number:16\n",
      "TRAIN Loss: 0.093894\n",
      "VALIDATION Loss: 0.260939\n",
      "Ensamble number:17\n",
      "TRAIN Loss: 0.099699\n",
      "VALIDATION Loss: 0.227333\n",
      "Ensamble number:18\n",
      "TRAIN Loss: 0.159274\n",
      "VALIDATION Loss: 0.247816\n",
      "Ensamble number:19\n",
      "TRAIN Loss: 0.158507\n",
      "VALIDATION Loss: 0.206099\n",
      "Ensamble number:20\n",
      "TRAIN Loss: 0.104228\n",
      "VALIDATION Loss: 0.345892\n",
      "Ensamble number:21\n",
      "TRAIN Loss: 0.087060\n",
      "VALIDATION Loss: 0.206634\n",
      "Ensamble number:22\n",
      "TRAIN Loss: 0.116211\n",
      "VALIDATION Loss: 0.215382\n",
      "Ensamble number:23\n",
      "TRAIN Loss: 0.097666\n",
      "VALIDATION Loss: 0.331629\n",
      "Ensamble number:24\n",
      "TRAIN Loss: 0.119246\n",
      "VALIDATION Loss: 0.250866\n",
      "Ensamble number:25\n",
      "TRAIN Loss: 0.116995\n",
      "VALIDATION Loss: 0.238538\n",
      "Ensamble number:26\n",
      "TRAIN Loss: 0.170327\n",
      "VALIDATION Loss: 0.262635\n",
      "Ensamble number:27\n",
      "TRAIN Loss: 0.106583\n",
      "VALIDATION Loss: 0.239727\n",
      "Ensamble number:28\n",
      "TRAIN Loss: 0.121312\n",
      "VALIDATION Loss: 0.427565\n",
      "Ensamble number:29\n",
      "TRAIN Loss: 0.117665\n",
      "VALIDATION Loss: 0.218318\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "for i in range (0 , 30):\n",
    "    model = models.SimpleNet()\n",
    "    # model = ResNetLike(BasicBlock, [1, 3, 3, 1], num_channels=2, num_classes=1)\n",
    "    print (\"Ensamble number:\" + str(i))\n",
    "    data, full_img = Iceberg_dataset.readSuffleData(seed_num=global_seed)\n",
    "    train_loader, val_loader, train_ds, val_ds = Iceberg_dataset.getTrainValLoaders(data,full_img,batch_size,num_workers)\n",
    "    # train_loader, val_loader, train_ds, val_ds = getCustomTrainValLoaders()\n",
    "    model, val_result=iceberg_classfier.generateSingleModel(model,train_loader, val_loader, train_ds, val_ds,num_epoches=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def savePred(df_pred, val_score):\n",
    "    csv_path = str(val_score) + '_sample_submission.csv'\n",
    "    df_pred.to_csv(csv_path, columns=('id', 'is_iceberg'), index=None)\n",
    "    print(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424, 4)\n",
      "0.218318_sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import iceberg_classfier\n",
    "df_pred = iceberg_classfier.testModel(model)\n",
    "savePred(df_pred, val_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
