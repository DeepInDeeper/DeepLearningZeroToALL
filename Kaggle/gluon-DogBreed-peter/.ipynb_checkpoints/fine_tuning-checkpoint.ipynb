{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "def reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir,\n",
    "                   valid_ratio):\n",
    "    # 读取训练数据标签。\n",
    "    with open(os.path.join(data_dir, label_file), 'r') as f:\n",
    "        # 跳过文件头行（栏名称）。\n",
    "        lines = f.readlines()[1:]\n",
    "        tokens = [l.rstrip().split(',') for l in lines]\n",
    "        idx_label = dict(((idx, label) for idx, label in tokens))\n",
    "    labels = set(idx_label.values())\n",
    "\n",
    "    num_train = len(os.listdir(os.path.join(data_dir, train_dir)))\n",
    "    # 训练集中数量最少一类的狗的数量。\n",
    "    min_num_train_per_label = (\n",
    "        Counter(idx_label.values()).most_common()[:-2:-1][0][1])\n",
    "    # 验证集中每类狗的数量。\n",
    "    num_valid_per_label = math.floor(min_num_train_per_label * valid_ratio)\n",
    "    label_count = dict()\n",
    "\n",
    "    def mkdir_if_not_exist(path):\n",
    "        if not os.path.exists(os.path.join(*path)):\n",
    "            os.makedirs(os.path.join(*path))\n",
    "\n",
    "    # 整理训练和验证集。\n",
    "    for train_file in os.listdir(os.path.join(data_dir, train_dir)):\n",
    "        idx = train_file.split('.')[0]\n",
    "        label = idx_label[idx]\n",
    "        mkdir_if_not_exist([data_dir, input_dir, 'train_valid', label])\n",
    "        shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                    os.path.join(data_dir, input_dir, 'train_valid', label))\n",
    "        if label not in label_count or label_count[label] < num_valid_per_label:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'valid', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'valid', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'train', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'train', label))\n",
    "\n",
    "    # 整理测试集。\n",
    "    mkdir_if_not_exist([data_dir, input_dir, 'test', 'unknown'])\n",
    "    for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n",
    "        shutil.copy(os.path.join(data_dir, test_dir, test_file),\n",
    "                    os.path.join(data_dir, input_dir, 'test', 'unknown'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = u\"/media/yijie/文档/dataset/ImageNetDog\"\n",
    "label_file = 'labels.csv'\n",
    "train_dir = 'train'\n",
    "test_dir = 'test'\n",
    "input_dir = 'train_valid_test'\n",
    "batch_size = 4\n",
    "valid_ratio = 0.1\n",
    "#reorg_dog_data(data_dir, label_file, train_dir, test_dir, input_dir,\n",
    "#                  valid_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.data import vision\n",
    "import numpy as np\n",
    "\n",
    "def transform_train(data, label):\n",
    "    im = image.imresize(data.astype('float32') / 255, 224, 224)\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 96, 96), resize=0,\n",
    "                        rand_crop=True, rand_resize=True, rand_mirror=True,\n",
    "                        mean=None, std=None,\n",
    "                        brightness=0, contrast=0,\n",
    "                        saturation=0, hue=0,\n",
    "                        pca_noise=0, rand_gray=0, inter_method=2)\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    # 将数据格式从\"高*宽*通道\"改为\"通道*高*宽\"。\n",
    "    im = nd.transpose(im, (2,0,1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))\n",
    "\n",
    "def transform_test(data, label):\n",
    "    im = image.imresize(data.astype('float32') / 255, 224, 224)\n",
    "    im = nd.transpose(im, (2,0,1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3092e2d64e80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                           last_batch='keep')\n\u001b[1;32m     18\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'keep'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m# 交叉熵损失函数。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0msoftmax_cross_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgluon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmaxCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object does not support indexing"
     ]
    }
   ],
   "source": [
    "input_str = \"/media/yijie/文档/dataset/ImageNetDog/train_valid_test/\"\n",
    "\n",
    "# 读取原始图像文件。flag=1说明输入图像有三个通道（彩色）。\n",
    "train_ds = vision.ImageFolderDataset(input_str + 'train', flag=1,\n",
    "                                     transform=transform_train)\n",
    "valid_ds = vision.ImageFolderDataset(input_str + 'valid', flag=1,\n",
    "                                     transform=transform_test)\n",
    "train_valid_ds = vision.ImageFolderDataset(input_str + 'train_valid',\n",
    "                                           flag=1, transform=transform_train)\n",
    "test_ds = vision.ImageFolderDataset(input_str + 'test', flag=1,\n",
    "                                     transform=transform_test)\n",
    "\n",
    "loader = gluon.data.DataLoader\n",
    "train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "valid_data = loader(valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "train_valid_data = loader(train_valid_ds, batch_size, shuffle=True,\n",
    "                          last_batch='keep')\n",
    "test_data = loader(test_ds, batch_size, shuffle=False, last_batch='keep')\n",
    "# 交叉熵损失函数。\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNetV1' object has no attribute 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c8c759ca6c46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfinetune_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet152_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfinetune_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfinetune_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXavier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNetV1' object has no attribute 'output'"
     ]
    }
   ],
   "source": [
    "from mxnet import init\n",
    "from mxnet.gluon.model_zoo import vision as models\n",
    "\n",
    "pretrained_net = models.resnet152_v1(pretrained=True)\n",
    "finetune_net = models.resnet152_v1(classes=120)\n",
    "finetune_net.features = pretrained_net.features\n",
    "finetune_net.output.initialize(init.Xavier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finetune_net.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import utils\n",
    "import mxnet as mx\n",
    "from mxnet import init\n",
    "from visual_visdom import Visualizer\n",
    "\n",
    "vis = Visualizer(env='demo')\n",
    "\n",
    "\n",
    "def train(net, ctx, train_data,batch_size=8, epochs=100, learning_rate=0.01, wd=0.001,print_batches=None):\n",
    "    vis.vis.env = 'demo'\n",
    "    net.collect_params().reset_ctx(ctx)\n",
    "    net.hybridize()\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {\n",
    "        'learning_rate': learning_rate, 'wd': wd})\n",
    "    \n",
    "    prev_time = datetime.datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        if epoch > 0 and epoch % 10 == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.4)\n",
    "        for data, label in train_data:\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(ctx))\n",
    "                loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "        cur_time = datetime.datetime.now()\n",
    "        \n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        Train_loss_vis = train_loss / len(train_data)\n",
    "        epoch_str = (\"Epoch %d. Train loss: %f, \"\n",
    "                         % (epoch, Train_loss_vis))\n",
    "        vis.plot('train_loss',Train_loss_vis)\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str + ', lr ' + str(trainer.learning_rate))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ctx = utils.try_gpu()\n",
    "train(finetune_net, ctx,train_valid_data)\n",
    "\n",
    "outputs = []\n",
    "for data, label in test_data:\n",
    "    output = nd.softmax(finetune_net(data.as_in_context(ctx)))\n",
    "    outputs.extend(output.asnumpy())\n",
    "ids = sorted(os.listdir(os.path.join(data_dir, input_dir, 'test/unknown')))\n",
    "with open('submission.csv', 'w') as f:\n",
    "    f.write('id,' + ','.join(train_valid_ds.synsets) + '\\n')\n",
    "    for i, output in zip(ids, outputs):\n",
    "        f.write(i.split('.')[0] + ',' + ','.join(\n",
    "            [str(num) for num in output]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
